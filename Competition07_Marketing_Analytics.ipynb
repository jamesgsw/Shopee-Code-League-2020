{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Contents:\n* [1. Import Relevant Libraries](#point_1)\n* [2. Reading in Datasets](#point_2)\n* [3. Feature Engineering and Data Cleaning](#point_3)\n    * [3.1 Inner Join Train and Test Dataset with User Dataset](#point_3_1)\n    * [3.2 Convert Datetime to Epoch](#point_3_2)\n    * [3.3 Finding Null values in Dataset](#point_3_3)\n    * [3.4 Change Values in last_open_day, last_login_day, last_checkout_day to 10^10](#point_3_4)\n    * [3.5 Ordinal Encoding Domain Column](#point_3_5)\n* [4. Univariate Analysis](#point_4)\n\t* [4.1 Country Code](#point_4_1)\n\t* [4.2 Subject Line Length](#point_4_2)\n\t* [4.3 last_open_day](#point_4_3)\n\t* [4.4 last_login_day](#point_4_4)\n\t* [4.5 last_checkout_day](#point_4_5)\n\t* [4.6 age](#point_4_6)\n\t* [4.7 Epoch Time](#point_4_7)\n\t* [4.8 domain_cat](#point_4_8)\n* [5. Correlation Matrix](#point_5)\n* [6. Bivariate Analysis](#point_6)\n* [7. Removing features based on Bivariate Analysis and Correlation Matrix](#point_7)\n* [8. Binning Continuous Variable - Age](#point_8)\n* [9. Encoding Catergorical Variables](#point_9)\n* [10. Ensuring Data Type in Correct Format](#point_10)\n* [11. Training Models](#point_11)\n* [12. Cross Validation of Model - Using K-Fold Cross Validation](#point_12)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"point_1\"> </a>\n# 1. Import Relevant Libraries","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Data Analysis\nimport time\nimport pandas as pd\nimport numpy as np\nimport scipy.stats as stats\n\n#Data Visualistion\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom plotly.tools import make_subplots\nfrom plotly.offline import iplot, init_notebook_mode\nimport plotly.express as px\nimport plotly.offline as py\nimport plotly.graph_objs as go\nfrom plotly.graph_objs import *\n\n#Data Pre Processing\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import LabelEncoder \nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder\nimport category_encoders as ce\n\n#Modeling\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\n\n#Validation\nfrom sklearn.model_selection import cross_val_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"point_2\"></a>\n# 2. Reading in Datasets","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/competition09marketing-analytics/train.csv\")\nusers_df = pd.read_csv(\"/kaggle/input/competition09marketing-analytics/users.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/competition09marketing-analytics/test.csv\")\nsubmission_df = pd.read_csv(\"/kaggle/input/competition09marketing-analytics/sample_submission_0_1.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"========== Training Dataset ===============\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"========== Test Dataset ===============\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"========== User Dataset ===============\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"users_df.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"========== Statistics of Training Dataset ===============\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"========== Statistics of Test Dataset ===============\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"========== Statistics of User Dataset ===============\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"users_df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"point_3\"></a>\n# 3. Feature Engineering and Data Cleaning","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"point_3_1\"></a>\n## 3.1 Inner Join Train and Test Dataset with User Dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.merge(train_df, users_df, how='inner', on=[\"user_id\"])\ntest_df = pd.merge(test_df, users_df, how='inner', on=[\"user_id\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"point_3_2\"></a>\n## 3.2 Convert Datetime to epoch","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def epoch_converter(date_time_string):\n    date_time_string\n    date_time_format = \"%Y-%m-%d %H:%M:%S%z\"\n    time_object = time.strptime(date_time_string, date_time_format)\n    return time.mktime(time_object)\n\ntrain_df[\"Epoch Time\"] = train_df[\"grass_date\"].apply(epoch_converter)\ntest_df[\"Epoch Time\"] = test_df[\"grass_date\"].apply(epoch_converter)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"point_3_3\"></a>\n## 3.3 Find null rows in dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"point_3_4\"></a>\n## 3.4 Change Values in last_open_day, last_login_day, last_checkout_day to 10**10","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[\"last_open_day\"] = train_df[\"last_open_day\"].apply(lambda x: 10**10 if x == \"Never open\" else x)\ntrain_df[\"last_login_day\"] = train_df[\"last_login_day\"].apply(lambda x: 10**10 if x == \"Never login\" else x)\ntrain_df[\"last_checkout_day\"] = train_df[\"last_checkout_day\"].apply(lambda x: 10**10 if x == \"Never checkout\" else x)\n\n\ntest_df[\"last_open_day\"] = test_df[\"last_open_day\"].apply(lambda x: 10**10 if x == \"Never open\" else x)\ntest_df[\"last_login_day\"] = test_df[\"last_login_day\"].apply(lambda x: 10**10 if x == \"Never login\" else x)\ntest_df[\"last_checkout_day\"] = test_df[\"last_checkout_day\"].apply(lambda x: 10**10 if x == \"Never checkout\" else x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"point_3_5\"></a>\n## 3.5 Ordinal Encoding Domain Column","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[\"domain\"] = train_df[\"domain\"].astype('category')\ntrain_df[\"domain_cat\"] = train_df[\"domain\"].cat.codes\n\ntest_df[\"domain\"] = test_df[\"domain\"].astype('category')\ntest_df[\"domain_cat\"] = test_df[\"domain\"].cat.codes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Removing Unecessary Columns\ntrain_df = train_df.drop(['user_id', 'domain', \"row_id\"], axis=1)\ntest_df = test_df.drop(['user_id', 'domain', \"row_id\"], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"point_3_6\"></a>\n## 3.6 Fill NaN Values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.isna().sum()\ntest_df.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = train_df.fillna(-1)\ntest_df = test_df.fillna(-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.isnull().sum()\ntest_df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"point_3_7\"></a>\n## 3.7 Time Catergorising","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def time_to_categorical_series(df,type=\"hour\"):\n    if type == \"hour\":\n        return df['date_time'].dt.hour.astype('category')\n    elif type == \"dayofweek\":\n        return df['date_time'].dt.dayofweek.astype('category')\n    elif type == \"month\":\n        return df['date_time'].dt.month.astype('category')\n    else:\n        return None\n    \ndef time_to_categorical(df):\n    hour_series = time_to_categorical_series(df,type='hour')\n    dayofweek_series = time_to_categorical_series(df,type='dayofweek')\n    month_series = time_to_categorical_series(df,type='month')\n\n    df['hour'] = hour_series\n    df['dayofweek'] = dayofweek_series\n    df['month'] = month_series","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['date_time'] = pd.to_datetime(train_df['grass_date'])\ntest_df['date_time'] = pd.to_datetime(test_df['grass_date'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"time_to_categorical(train_df)\ntime_to_categorical(test_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.to_csv(\"train_df_cleaned.csv\")\ntest_df.to_csv(\"test_df_cleaned.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"point_4\"></a>\n# 4. Univariate Analysis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plotting Histogram\ndef plotHistogram(variable):\n    \"\"\"Plots histogram and density plot of a variable.\"\"\"\n    \n    # Create subplot object.\n    fig = make_subplots(\n        rows=2,\n        cols=1,\n        print_grid=False,\n    subplot_titles=(f\"Distribution of {variable.name} with Histogram\", f\"Distribution of {variable.name} with Density Plot\"))\n    \n    # This is a count histogram\n    fig.add_trace(\n        go.Histogram(\n            x = variable,\n            hoverinfo=\"x+y\",\n            marker = dict(color = \"chocolate\")\n        ),\n    row=1,col=1)\n    \n    # This is a density histogram\n    fig.add_trace(\n        go.Histogram(\n            x = variable,\n            hoverinfo=\"x+y\",\n            histnorm = \"density\",\n            marker = dict(color = \"darkred\")\n        ),\n    row=2,col=1)\n    \n    # Update layout\n    fig.layout.update(\n        height=400, \n        width=800,\n        hovermode=\"closest\",\n        showlegend=False,\n        paper_bgcolor=\"rgb(243, 243, 243)\",\n        plot_bgcolor=\"rgb(243, 243, 243)\"\n        )\n    \n    # Update axes\n    fig.layout.yaxis1.update(title=\"<b>Abs Frequency</b>\")\n    fig.layout.yaxis2.update(title=\"<b>Density(%)</b>\")\n    fig.layout.xaxis2.update(title=f\"<b>{variable.name}</b>\")\n    return fig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"point_4_1\"></a>\n## 4.1 Country Code","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_Count = train_df['country_code'].value_counts().rename_axis('country_code').reset_index(name='Count')\ndf_Count\n\n#Plotly Style\nfig = px.histogram(df_Count, x=\"country_code\", y='Count',nbins=50)\nfig.update_layout(height=300)\nfig.show('notebook')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"point_4_2\"></a>\n## 4.2 Subject Line Length","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_Count = train_df['subject_line_length'].value_counts().rename_axis('subject_line_length').reset_index(name='Count')\ndf_Count\n\n#Plotly Style\nfig = px.histogram(df_Count, x=\"subject_line_length\", y='Count',nbins=50)\nfig.update_layout(height=300)\nfig.show('notebook')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"point_4_3\"></a>\n## 4.3 last_open_day","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_Count = train_df['last_open_day'].value_counts().rename_axis('last_open_day').reset_index(name='Count')\ndf_Count\n\n#Plotly Style\nfig = px.histogram(df_Count, x=\"last_open_day\", y='Count',nbins=50)\nfig.update_layout(height=300)\nfig.show('notebook')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"point_4_4\"></a>\n## 4.4 last_login_day","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_Count = train_df['last_login_day'].value_counts().rename_axis('last_login_day').reset_index(name='Count')\ndf_Count\n\n#Plotly Style\nfig = px.histogram(df_Count, x=\"last_login_day\", y='Count',nbins=50)\nfig.update_layout(height=300)\nfig.show('notebook')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"point_4_5\"></a>\n## 4.5 last_checkout_day","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_Count = train_df['last_checkout_day'].value_counts().rename_axis('last_checkout_day').reset_index(name='Count')\ndf_Count\n\n#Plotly Style\nfig = px.histogram(df_Count, x=\"last_checkout_day\", y='Count',nbins=50)\nfig.update_layout(height=300)\nfig.show('notebook')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"point_4_6\"></a>\n## 4.6 age","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_Count = train_df['age'].value_counts().rename_axis('age').reset_index(name='Count')\ndf_Count\n\n#Plotly Style\nfig = px.histogram(df_Count, x=\"age\", y='Count',nbins=50)\nfig.update_layout(height=300)\nfig.show('notebook')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"point_4_7\"></a>\n## 4.7 Epoch Time","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_Count = train_df['Epoch Time'].value_counts().rename_axis('Epoch Time').reset_index(name='Count')\ndf_Count\n\n#Plotly Style\nfig = px.histogram(df_Count, x=\"Epoch Time\", y='Count',nbins=100)\nfig.update_layout(height=300)\nfig.show('notebook')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"point_4_8\"></a>\n## 4.8 domain_cat","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_Count = train_df['domain_cat'].value_counts().rename_axis('domain_cat').reset_index(name='Count')\ndf_Count\n\n#Plotly Style\nfig = px.histogram(df_Count, x=\"domain_cat\", y='Count',nbins=50)\nfig.update_layout(height=300)\nfig.show('notebook')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"point_5\"></a>\n# 5. Correlation Matrix\nPerforming the correlation matrix, we intend to identify the MultiCollinearity Problem. Where we want to find features where the tolerance is less than 0.2, which would indicate MultiCollinearity Problem. Formula: Tolerance = 1 - R**2","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xTrain = train_df.drop(columns=[\"open_flag\"])\n\n#Checking for collinearity\npearsoncorr = xTrain.corr(method='pearson')\n#Styling\nplt.figure(figsize=(30, 10))\nsns.heatmap(pearsoncorr, \n            xticklabels=pearsoncorr.columns,\n            yticklabels=pearsoncorr.columns,\n            cmap='RdBu_r',\n            annot=True,\n            linewidth=0.3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Findings:** We find that there's high correlation between open_count_last_xx_days features, login_count_last_xx_days features and checkout_count_last_xx_days features. Therefore we would want to remove all but one to overcome multicollinearity problem.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"point_6\"></a>\n# 6. Bivariate Analysis\nCatergorical variable: Country Code, domain_cat <br/>\nContinuous Variable: subject_line_length, last_open_day, last_login_day, last_checkout_day, open_count_last_10_days, open_count_last_30_days, open_count_last_60_days, login_count_last_10_days, login_count_last_30_days, login_count_last_60_days, checkout_count_last_10_days, checkout_count_last_30_days, checkout_count_last_60_days, attr_1, attr_2, attr_3, age, Epoch Time <br/>\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"point_6_1\"></a>\n## 6.1 ANOVA Test for Continuous Variable","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"continuous_variable_list = ['subject_line_length','last_open_day','last_login_day','last_checkout_day','open_count_last_10_days','open_count_last_30_days','open_count_last_60_days', 'login_count_last_10_days',\n                            'login_count_last_30_days','login_count_last_60_days','checkout_count_last_10_days','checkout_count_last_30_days','checkout_count_last_60_days','attr_1','attr_2','attr_3',\n                            'age','Epoch Time']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"anova_dict = {}\nfor that_column in continuous_variable_list:\n    \n    numVariable = train_df[that_column]\n    catVariable = train_df[\"open_flag\"]\n    #Seperating into the 2 different population dataset\n    groupNumVariableByCatVariable0 = numVariable[catVariable == 0]\n    groupNumVariableByCatVariable1 = numVariable[catVariable == 1]\n\n    fValue, pValue = stats.f_oneway(groupNumVariableByCatVariable0, groupNumVariableByCatVariable1)\n   \n    anova_dict[that_column] = pValue","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"anova_dict","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"point_6_2\"></a>\n## 6.2 Chi_Squared Test for Discrete Variable","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"discrete_variable_list = [\"country_code\", \"domain_cat\"]\n\nchi_sq_dict = {}\nfor that_column in discrete_variable_list:\n    \n    X = train_df[that_column]\n    y = train_df[\"open_flag\"]\n\n    array_by_open_flag = pd.crosstab(index = X, columns = y)\n\n    chi2_stat, p_val, dof, ex = stats.chi2_contingency(array_by_open_flag)\n   \n    chi_sq_dict[that_column] = p_val","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"chi_sq_dict","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"point_7\"></a>\n# 7. Removing features based on Bivariate Analysis and Correlation Matrix","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = train_df.drop(columns=[\"grass_date\", \"open_count_last_30_days\",\"open_count_last_60_days\",\"login_count_last_30_days\",\"login_count_last_60_days\",\"checkout_count_last_30_days\",\"checkout_count_last_60_days\", \"Epoch Time\", \"date_time\"])\ntest_df = test_df.drop([\"grass_date\", \"open_count_last_30_days\",\"open_count_last_60_days\",\"login_count_last_30_days\",\"login_count_last_60_days\",\"checkout_count_last_30_days\",\"checkout_count_last_60_days\", \"Epoch Time\", \"date_time\"], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"point_8\"></a>\n# 8. Binning Continuous Variable - Age","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.age.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create bin categories for Age.\nageGroups = [1,2,3,4,5]\n\n#Create range for each bin categories of Age.\ngroupRanges = [-17,0,23,31,40,118]\n\n#Create and view categorized Age with original Age.\ntrain_df[\"Age Binned\"] = pd.cut(train_df.age, groupRanges, labels = ageGroups)\ntest_df[\"Age Binned\"] = pd.cut(test_df.age, groupRanges, labels = ageGroups)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = train_df.drop(columns=[\"age\"])\ntest_df = test_df.drop(columns=[\"age\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"point_9\"></a>\n# 9. Encoding Catergorical Variables","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder = ce.BinaryEncoder(cols=[\"domain_cat\"])\ndf_bin = encoder.fit_transform(train_df['domain_cat'])\ntrain_df = pd.concat([train_df, df_bin], axis=1)\n\nencoder = ce.BinaryEncoder(cols=[\"domain_cat\"])\ndf_bin = encoder.fit_transform(test_df['domain_cat'])\ntest_df = pd.concat([test_df, df_bin], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder = ce.BinaryEncoder(cols=[\"country_code\"])\ndf_bin = encoder.fit_transform(train_df['country_code'])\ntrain_df = pd.concat([train_df, df_bin], axis=1)\n\nencoder = ce.BinaryEncoder(cols=[\"country_code\"])\ndf_bin = encoder.fit_transform(test_df['country_code'])\ntest_df = pd.concat([test_df, df_bin], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder = ce.BinaryEncoder(cols=[\"hour\"])\ndf_bin = encoder.fit_transform(train_df['hour'])\ntrain_df = pd.concat([train_df, df_bin], axis=1)\n\nencoder = ce.BinaryEncoder(cols=[\"hour\"])\ndf_bin = encoder.fit_transform(test_df['hour'])\ntest_df = pd.concat([test_df, df_bin], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder = ce.BinaryEncoder(cols=[\"dayofweek\"])\ndf_bin = encoder.fit_transform(train_df['dayofweek'])\ntrain_df = pd.concat([train_df, df_bin], axis=1)\n\nencoder = ce.BinaryEncoder(cols=[\"dayofweek\"])\ndf_bin = encoder.fit_transform(test_df['dayofweek'])\ntest_df = pd.concat([test_df, df_bin], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# encoder = ce.BinaryEncoder(cols=[\"month\"])\n# df_bin = encoder.fit_transform(train_df['month'])\n# train_df = pd.concat([train_df, df_bin], axis=1)\n\n# encoder = ce.BinaryEncoder(cols=[\"month\"])\n# df_bin = encoder.fit_transform(test_df['month'])\n# test_df = pd.concat([test_df, df_bin], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder = ce.BinaryEncoder(cols=[\"Age Binned\"])\ndf_bin = encoder.fit_transform(train_df['Age Binned'])\ntrain_df = pd.concat([train_df, df_bin], axis=1)\n\nencoder = ce.BinaryEncoder(cols=[\"Age Binned\"])\ndf_bin = encoder.fit_transform(test_df['Age Binned'])\ntest_df = pd.concat([test_df, df_bin], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = train_df.drop(columns=[\"country_code\",\"domain_cat\",\"hour\",\"dayofweek\",\"Age Binned\"])\n\ntest_df = test_df.drop(columns=[\"country_code\",\"domain_cat\",\"hour\",\"dayofweek\",\"Age Binned\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"point_10\"></a>\n# 10. Ensuring Data Type in Correct Format","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[\"last_open_day\"] = train_df[\"last_open_day\"].astype('int64')\ntrain_df[\"last_login_day\"] = train_df[\"last_login_day\"].astype('int64')\ntrain_df[\"last_checkout_day\"] = train_df[\"last_checkout_day\"].astype('int64')\ntrain_df[\"month\"] = train_df[\"month\"].astype('int64')\n\ntest_df[\"last_open_day\"] = test_df[\"last_open_day\"].astype('int64')\ntest_df[\"last_login_day\"] = test_df[\"last_login_day\"].astype('int64')\ntest_df[\"last_checkout_day\"] = test_df[\"last_checkout_day\"].astype('int64')\ntest_df[\"month\"] = test_df[\"month\"].astype('int64')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#final_check\ntrain_df.isna().sum()\ntest_df.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"point_11\"></a>\n# 11. Training Models","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"Gradient Boosting Classifier\"\"\"\ngbc = GradientBoostingClassifier(random_state = 43)\n\n\n\"\"\"#10.Extreme Gradient Boosting\"\"\"\nxgbc = XGBClassifier()\n\n\"\"\"List of all the models with their indices.\"\"\"\nmodelNames = [\"GBC\",\"XGBC\"]\nmodels = [gbc, xgbc]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"yTrain = train_df[[\"open_flag\"]].values.ravel()\nxTrain = train_df.drop(columns=[\"open_flag\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xTrain.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def calculateTrainAccuracy(model):\n    \"\"\"Returns training accuracy of a model.\"\"\"\n    model.fit(xTrain, yTrain)\n    trainAccuracy = model.score(xTrain, yTrain)\n    trainAccuracy = round(trainAccuracy*100, 2)\n    return trainAccuracy\n\n# Calculate train accuracy of all the models and store them in a dataframe\nmodelScores = list(map(calculateTrainAccuracy, models))\ntrainAccuracy = pd.DataFrame(modelScores, columns = [\"trainAccuracy\"], index=modelNames)\ntrainAccuracySorted = trainAccuracy.sort_values(by=\"trainAccuracy\", ascending=False)\nprint(\"~~~~~~~ Training Accuracy of the Classifiers ~~~~~~~~~~~\")\nprint(trainAccuracySorted)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"point_12\"></a>\n# 12. Cross Validation of Model - Using K-Fold Cross Validation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def calculateXValScore(model):\n    \"\"\"Returns models' cross validation scores.\"\"\"\n    \n    xValScore = cross_val_score(model, xTrain, yTrain, cv = 10, scoring=\"accuracy\").mean()\n    xValScore = round(xValScore*100, 2)\n    return xValScore\n\n# Calculate cross validation scores of all the models and store them in a dataframe\nmodelScores = list(map(calculateXValScore, models))\nxValScores = pd.DataFrame(modelScores, columns = [\"xValScore\"], index=modelNames)\nxValScoresSorted = xValScores.sort_values(by=\"xValScore\", ascending=False)\n\ndisplay(xValScoresSorted)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Save model\n# import pickle\n# modelNames_list = [\"LR\", \"SVC\", \"RF\", \"KNN\", \"GNB\", \"DT\", \"GBC\", \"ABC\", \"ETC\", \"XGBC\"]\n# models_list = [lr, svc, rf, knn, gnb, dt, gbc, abc, etc, xgbc]\n\n# for model, modelName in zip(models_list, modelNames_list):\n#     model_filename = modelName + \".sav\"\n#     pickle.dump(model, open(model_filename, \"wb\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# #Read model\n# import pickle\n\n# model = pickle.load(open(\"ETC.sav\", \"rb\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(xTrain.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(test_df.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"yPred = xgbc.predict(test_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"yPred_df = pd.DataFrame(data=yPred, columns=[\"open_flag\"])\nyPred_df.insert(0, 'row_id', range(0, len(yPred_df)))\nyPred_df.reset_index(drop=True, inplace=True)\nyPred_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(yPred_df.index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"yPred_df.to_csv(\"yPred_new.csv\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}