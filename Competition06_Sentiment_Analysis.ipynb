{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Competition #06 - Sentiment Analysis","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input/competition-06-dataset/'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1. Importing Relevant Libraries","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport spacy\nfrom spacy.lang.en import English\nfrom spacy.lang.en.stop_words import STOP_WORDS\nimport re","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Reading in Datasets","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/competition-06-dataset/train_nlp.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/competition-06-dataset/test_nlp.csv\")\nsampleSubmission_df = pd.read_csv(\"/kaggle/input/competition-06-dataset/sampleSubmission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sampleSubmission_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Setting dataset to perform datacleaning","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = train_df#.sample(n=50, replace=False, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Removing Emoji","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def deEmojify(text):\n    regrex_pattern = re.compile(pattern = \"[\"\n        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           \"]+\", flags = re.UNICODE)\n    return regrex_pattern.sub(r'',text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset['review'] = dataset['review'].apply(deEmojify)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Word Tokeninzing the Reviews","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load English tokenizer, tagger, parser, NER and word vectors\nnlp = spacy.load(\"en_core_web_sm\")\n\ndef word_tokenizer(text):\n    my_doc = nlp(text)\n    token_list = []\n    for token in my_doc:\n        token_list.append(token.text)\n    return(token_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset['Word Tokenized Review'] = dataset['review'].apply(word_tokenizer)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4B. Sentence Tokenizing","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# nlp = spacy.load(\"en_core_web_sm\")\n\n# def sentence_tokenizer(text):\n#     # Create the pipeline 'sentencizer' component\n#     sbd = nlp.create_pipe('sentencizer')\n#     # Add the component to the pipeline\n#     nlp.add_pipe(sbd)\n#     #  \"nlp\" Object is used to create documents with linguistic annotations.\n#     doc = nlp(text)\n#     # create list of sentence tokens\n#     sents_list = []\n#     for sent in doc.sents:\n#         sents_list.append(sent.text)\n#     return(sents_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# dataset['Sentence Tokenized Review'] = dataset['review'].apply(sentence_tokenizer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5. Removing Stop Words\nStop words are like glue that helps us bond a set of words into an intepretable speech. But it's not really that useful, therefore we can remove them.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def stop_words_removal(text):\n    filtered_value = []\n    spacy_stopwords = list(spacy.lang.en.stop_words.STOP_WORDS)\n    for word in text:\n        if not word in spacy_stopwords:\n            filtered_value.append(word)\n    return(filtered_value)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset['Word Tokenized Review w/o StopWords'] = dataset['Word Tokenized Review'].apply(stop_words_removal)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6. Lemmatization","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"nlp = spacy.load(\"en_core_web_sm\")\ndef alemantiser(text):\n    alist = []\n    for word in text:\n        sometext = nlp(word)\n        for avalue in sometext:\n            lem_text = avalue.lemma_\n            alist.append(lem_text)\n    return alist","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset['Word Tokenized Review w/o StopWords w Lemantised'] = dataset['Word Tokenized Review w/o StopWords'].apply(alemantiser)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 7. Vectorising Language","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def join_back(x):\n    separator = ' '\n    return(separator.join(x).strip())\n\ndataset['Joined Cleaned Review'] = dataset['Word Tokenized Review w/o StopWords w Lemantised'].apply(join_back)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.to_csv(\"Joined Cleaned Review.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We just want the vectors so we can turn off other models in the pipeline\nwith nlp.disable_pipes():\n    vectors = np.array([nlp(review['Joined Cleaned Review']).vector for idx, review in dataset.iterrows()])\n    \nvectors.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.savetxt('train_data_vectors.txt', vectors)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#b = np.loadtxt(\"/kaggle/input/competition-06-dataset/train_data_vectors.txt\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 8. Training the Model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 8a. LinearSVC Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import LinearSVC\n\ny_train = train_df.rating[:200]\n\n# Create the LinearSVC model\nmodel = LinearSVC(random_state=1, dual=False, max_iter=10000)\n# Fit the model\nmodel.fit(vectors, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 8b. XGBoost Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scratch space in case you want to experiment with other models\nimport xgboost as xgb\nxg_reg = xgb.XGBRegressor(objective ='reg:linear', colsample_bytree = 0.3, learning_rate = 0.1,\n                max_depth = 5, alpha = 10, n_estimators = 10)\n\nxg_reg.fit(vectors, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 9. Predicting","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df[\"rating\"] = model.predict(test_df.review)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df[\"rating\"] = xg_reg.predict(review)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}